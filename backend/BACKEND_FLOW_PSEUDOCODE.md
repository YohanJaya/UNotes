# ğŸ”„ Backend Flow Diagrams & Pseudocode

## Complete Request-Response Flow for UNotes AI System

---

## ğŸ“Š HIGH-LEVEL FLOW DIAGRAM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND                                 â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Slide Change    â”‚              â”‚  User Types      â”‚        â”‚
â”‚  â”‚  Event Detected  â”‚              â”‚  Question        â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚                                  â”‚                   â”‚
â”‚           â–¼                                  â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Build AUTO_MODE  â”‚              â”‚ Build CHAT_MODE  â”‚        â”‚
â”‚  â”‚ Request Payload  â”‚              â”‚ Request Payload  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚                                  â”‚                   â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                          â”‚                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     POST /api/chat                     â”‚
        â”‚     server.js                          â”‚
        â”‚                                        â”‚
        â”‚  1. Receive request                    â”‚
        â”‚  2. Extract body                       â”‚
        â”‚  3. Call handleAIChat()                â”‚
        â”‚  4. Return response                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     handleAIChat()                     â”‚
        â”‚     Endpoints/ai.js                    â”‚
        â”‚                                        â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
        â”‚  â”‚  Mode Detection Logic    â”‚         â”‚
        â”‚  â”‚                          â”‚         â”‚
        â”‚  â”‚  if (mode === 'AUTO')    â”‚         â”‚
        â”‚  â”‚    â†’ handleAutoSlide...()â”‚         â”‚
        â”‚  â”‚                          â”‚         â”‚
        â”‚  â”‚  else (mode === 'CHAT')  â”‚         â”‚
        â”‚  â”‚    â†’ handleManualChat()  â”‚         â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                  â”‚
          â–¼                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ handleAutoSlide     â”‚          â”‚ handleManualChat()   â”‚
â”‚ Explanation()       â”‚          â”‚                      â”‚
â”‚                     â”‚          â”‚                      â”‚
â”‚ 1. Validate imageUrlâ”‚          â”‚ 1. Validate question â”‚
â”‚ 2. Build AUTO promptâ”‚          â”‚ 2. Build CHAT prompt â”‚
â”‚ 3. Add slide contextâ”‚          â”‚ 3. Add soft context  â”‚
â”‚ 4. Add notes (soft) â”‚          â”‚ 4. Choose model      â”‚
â”‚ 5. Call GPT-4 Visionâ”‚          â”‚ 5. Call OpenAI API   â”‚
â”‚ 6. Return teaching  â”‚          â”‚ 6. Return response   â”‚
â”‚    explanation      â”‚          â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                 â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     OpenAI API                         â”‚
        â”‚                                        â”‚
        â”‚  GPT-4 Vision (for slides)             â”‚
        â”‚  GPT-4 Turbo (for text reasoning)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Response Assembly                  â”‚
        â”‚                                        â”‚
        â”‚  {                                     â”‚
        â”‚    response: "...",                    â”‚
        â”‚    mode: "AUTO_MODE" | "CHAT_MODE",    â”‚
        â”‚    timestamp: "..."                    â”‚
        â”‚  }                                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Return to Frontend                 â”‚
        â”‚                                        â”‚
        â”‚  Display in appropriate UI style       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» DETAILED PSEUDOCODE

### 1. Server Entry Point (`server.js`)

```pseudocode
FUNCTION handleChatEndpoint(request, response):
    TRY:
        // Extract request body
        requestData = parseJSON(request.body)
        
        // Delegate to AI logic handler
        result = await handleAIChat(requestData)
        
        // Send success response
        response.json(result)
        
    CATCH error:
        IF error.message == 'Question is required':
            response.status(400).json({ error: error.message })
        ELSE IF error.message == 'AUTO_MODE requires slide image URL':
            response.status(400).json({ error: error.message })
        ELSE:
            log(error)
            response.status(500).json({ 
                error: 'Failed to get AI response',
                details: error.message 
            })
    END TRY
END FUNCTION
```

---

### 2. Main Router (`handleAIChat()`)

```pseudocode
FUNCTION handleAIChat(requestData):
    // Extract all possible fields
    mode = requestData.mode
    userQuestion = requestData.userQuestion OR requestData.question
    imageUrl = requestData.imageUrl
    slideText = requestData.slideText
    slideIndex = requestData.slideIndex
    notes = requestData.notes
    
    // Legacy support
    isSlideAnalysis = requestData.isSlideAnalysis
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // MODE DETECTION LOGIC (CRITICAL)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    detectedMode = NULL
    
    // Priority 1: Explicit mode (RECOMMENDED)
    IF mode IS PROVIDED:
        detectedMode = mode
        
    // Priority 2: Legacy flag
    ELSE IF isSlideAnalysis == TRUE:
        detectedMode = 'AUTO_MODE'
        
    // Priority 3: Auto-inference (imageUrl but no question)
    ELSE IF imageUrl EXISTS AND NOT userQuestion:
        detectedMode = 'AUTO_MODE'
        
    // Priority 4: User question provided
    ELSE IF userQuestion EXISTS:
        detectedMode = 'CHAT_MODE'
        
    // Error: Ambiguous
    ELSE:
        THROW Error('Cannot determine AI mode')
    END IF
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ROUTE TO APPROPRIATE HANDLER
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    aiResponse = NULL
    
    IF detectedMode == 'AUTO_MODE':
        // Automatic slide explanation
        aiResponse = await handleAutoSlideExplanation({
            imageUrl: imageUrl,
            slideText: slideText,
            slideIndex: slideIndex,
            notes: notes
        })
        
    ELSE IF detectedMode == 'CHAT_MODE':
        // Manual user question
        aiResponse = await handleManualChat({
            userQuestion: userQuestion,
            notes: notes,
            slideText: slideText,
            slideIndex: slideIndex,
            imageUrl: imageUrl  // Optional context
        })
        
    ELSE:
        THROW Error('Invalid mode: ' + detectedMode)
    END IF
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ASSEMBLE AND RETURN RESPONSE
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    RETURN {
        response: aiResponse,
        mode: detectedMode,
        timestamp: getCurrentTimestamp()
    }
END FUNCTION
```

---

### 3. AUTO_MODE Handler

```pseudocode
FUNCTION handleAutoSlideExplanation(params):
    imageUrl = params.imageUrl
    slideText = params.slideText
    slideIndex = params.slideIndex
    notes = params.notes
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // VALIDATION
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    IF NOT imageUrl:
        THROW Error('AUTO_MODE requires slide image URL')
    END IF
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // BUILD SYSTEM PROMPT (LECTURER MODE)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    systemPrompt = AUTO_MODE_PROMPT  // Pre-defined template
    
    // Add soft context (notes as reference, NOT restriction)
    IF notes AND notes.length > 0:
        notesContext = buildNotesContext(notes)
        systemPrompt = systemPrompt + notesContext
    END IF
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // BUILD SLIDE CONTEXT
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    slideContext = ""
    slideContext += "\n\nğŸ“Š SLIDE INFORMATION:\n"
    slideContext += "Slide Number: " + (slideIndex OR "Current") + "\n"
    
    IF slideText:
        slideContext += "Slide Content:\n" + slideText + "\n"
    END IF
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // BUILD AUTO INSTRUCTION (NO USER INPUT)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    autoInstruction = slideContext + "\n\n"
    autoInstruction += "**TASK:** Explain this slide in detail "
    autoInstruction += "as if teaching it in a live lecture. "
    autoInstruction += "The student has just navigated to this slide "
    autoInstruction += "and needs to understand it thoroughly. "
    autoInstruction += "Provide a comprehensive explanation that goes "
    autoInstruction += "beyond what's visible on the slide."
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // CONSTRUCT OPENAI API MESSAGES
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    messages = [
        {
            role: "system",
            content: systemPrompt
        },
        {
            role: "user",
            content: [
                {
                    type: "text",
                    text: autoInstruction
                },
                {
                    type: "image_url",
                    image_url: {
                        url: imageUrl,
                        detail: "high"  // High detail for OCR
                    }
                }
            ]
        }
    ]
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // CALL GPT-4 VISION
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    completion = await openai.chat.completions.create({
        model: "gpt-4-vision-preview",
        messages: messages,
        max_tokens: 1500,        // Higher for teaching
        temperature: 0.7         // Balanced
    })
    
    aiResponse = completion.choices[0].message.content
    
    RETURN aiResponse
END FUNCTION
```

---

### 4. CHAT_MODE Handler

```pseudocode
FUNCTION handleManualChat(params):
    userQuestion = params.userQuestion
    notes = params.notes
    slideText = params.slideText
    slideIndex = params.slideIndex
    imageUrl = params.imageUrl
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // VALIDATION
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    IF NOT userQuestion OR userQuestion.trim() == "":
        THROW Error('CHAT_MODE requires a user question')
    END IF
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // BUILD SYSTEM PROMPT (TUTOR MODE)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    systemPrompt = CHAT_MODE_PROMPT  // Pre-defined template
    
    // Add soft context (notes as reference, NOT boundary)
    IF notes AND notes.length > 0:
        notesContext = buildNotesContext(notes)
        systemPrompt = systemPrompt + notesContext
    END IF
    
    // Add current slide context (optional, soft)
    IF slideText OR slideIndex:
        systemPrompt += "\n\nğŸ“Š CURRENT CONTEXT:\n"
        systemPrompt += "The student is currently viewing "
        systemPrompt += (slideIndex ? "Slide " + slideIndex : "a lecture slide")
        systemPrompt += ".\n"
        
        IF slideText:
            systemPrompt += "Slide content: " + slideText + "\n"
        END IF
        
        systemPrompt += "This is just for context - "
        systemPrompt += "feel free to answer beyond this material.\n"
    END IF
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // CHOOSE MODEL BASED ON VISUAL NEEDS
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    messages = []
    completion = NULL
    
    IF imageUrl EXISTS:
        // User might be asking about visible slide
        // Use GPT-4 Vision
        
        messages = [
            {
                role: "system",
                content: systemPrompt
            },
            {
                role: "user",
                content: [
                    {
                        type: "text",
                        text: userQuestion
                    },
                    {
                        type: "image_url",
                        image_url: {
                            url: imageUrl,
                            detail: "high"
                        }
                    }
                ]
            }
        ]
        
        completion = await openai.chat.completions.create({
            model: "gpt-4-vision-preview",
            messages: messages,
            max_tokens: 1200,
            temperature: 0.8  // Higher creativity
        })
        
    ELSE:
        // Text-only question
        // Use GPT-4 Turbo (best reasoning)
        
        messages = [
            {
                role: "system",
                content: systemPrompt
            },
            {
                role: "user",
                content: userQuestion
            }
        ]
        
        completion = await openai.chat.completions.create({
            model: "gpt-4-turbo-preview",
            messages: messages,
            max_tokens: 1200,
            temperature: 0.8
        })
    END IF
    
    aiResponse = completion.choices[0].message.content
    
    RETURN aiResponse
END FUNCTION
```

---

### 5. Context Builders (Utilities)

```pseudocode
FUNCTION buildNotesContext(notes):
    IF NOT notes OR notes.length == 0:
        RETURN ""
    END IF
    
    context = "\n\nğŸ“š **REFERENCE MATERIALS (Student's Notes):**\n"
    context += "The student has taken these notes. "
    context += "Use them as context but feel free to expand beyond them.\n\n"
    
    FOR EACH note IN notes:
        context += "Note " + (index + 1) + ": " + note.title + "\n"
        
        IF note.content AND note.content IS ARRAY:
            context += note.content.join(' ') + "\n"
        END IF
        
        IF note.tags AND note.tags.length > 0:
            context += "Topics: " + note.tags.join(', ') + "\n"
        END IF
        
        context += "\n"
    END FOR
    
    RETURN context
END FUNCTION

FUNCTION buildSlideContext(slideText, slideIndex):
    context = "\n\nğŸ“Š **SLIDE INFORMATION:**\n"
    context += "Slide Number: " + (slideIndex OR "Current") + "\n"
    
    IF slideText:
        context += "Slide Content:\n" + slideText + "\n"
    END IF
    
    RETURN context
END FUNCTION
```

---

## ğŸ”€ MODE DETECTION DECISION TREE

```
START
  â”‚
  â”œâ”€ Is 'mode' explicitly provided?
  â”‚    â”œâ”€ YES â†’ Use provided mode
  â”‚    â””â”€ NO â†“
  â”‚
  â”œâ”€ Is 'isSlideAnalysis' TRUE? (legacy)
  â”‚    â”œâ”€ YES â†’ AUTO_MODE
  â”‚    â””â”€ NO â†“
  â”‚
  â”œâ”€ Is 'imageUrl' provided BUT NO 'userQuestion'?
  â”‚    â”œâ”€ YES â†’ AUTO_MODE (auto-trigger inference)
  â”‚    â””â”€ NO â†“
  â”‚
  â”œâ”€ Is 'userQuestion' provided?
  â”‚    â”œâ”€ YES â†’ CHAT_MODE
  â”‚    â””â”€ NO â†“
  â”‚
  â””â”€ ERROR: Cannot determine mode
```

---

## ğŸ¯ REQUEST VALIDATION MATRIX

| Mode | Required Fields | Optional Fields | Validation Error |
|------|----------------|-----------------|------------------|
| **AUTO_MODE** | `imageUrl` | `slideText`, `slideIndex`, `notes` | "AUTO_MODE requires slide image URL" |
| **CHAT_MODE** | `userQuestion` | `imageUrl`, `slideText`, `slideIndex`, `notes` | "CHAT_MODE requires a user question" |

---

## ğŸ”„ RESPONSE FLOW TIMING

```
Frontend Event
    â”‚
    â”œâ”€ 0ms: User action (slide change or question submit)
    â”œâ”€ 10ms: Build request payload
    â”œâ”€ 20ms: Send HTTP POST to /api/chat
    â”‚
    â–¼
Backend Processing
    â”‚
    â”œâ”€ 50ms: Receive request, parse JSON
    â”œâ”€ 55ms: Mode detection
    â”œâ”€ 60ms: Route to handler
    â”œâ”€ 65ms: Build system prompt
    â”œâ”€ 70ms: Construct OpenAI messages
    â”œâ”€ 100ms: Send to OpenAI API
    â”‚
    â–¼
OpenAI Processing
    â”‚
    â”œâ”€ 100-3000ms: GPT-4 Vision processing (AUTO_MODE)
    â”œâ”€ 100-2000ms: GPT-4 Turbo processing (CHAT_MODE)
    â”‚
    â–¼
Response Assembly
    â”‚
    â”œâ”€ +10ms: Extract completion content
    â”œâ”€ +15ms: Build response object
    â”œâ”€ +20ms: Send JSON back to frontend
    â”‚
    â–¼
Frontend Display
    â”‚
    â”œâ”€ +30ms: Parse response
    â”œâ”€ +35ms: Add to messages state
    â”œâ”€ +40ms: Render in UI
    â”‚
    â–¼
Total Time: 150ms-3100ms (typical: 1500ms)
```

---

## ğŸ›¡ï¸ ERROR HANDLING FLOW

```pseudocode
TRY:
    result = handleAIChat(requestData)
    RETURN success(result)
    
CATCH ValidationError (e.g., missing field):
    LOG error
    RETURN status(400).json({
        error: error.message,
        type: 'validation',
        field: error.field
    })
    
CATCH OpenAIError (e.g., API failure):
    LOG error
    RETURN status(500).json({
        error: 'OpenAI API error',
        type: 'openai',
        details: error.message
    })
    
CATCH NetworkError:
    LOG error
    RETURN status(503).json({
        error: 'Service temporarily unavailable',
        type: 'network'
    })
    
CATCH UnknownError:
    LOG error
    RETURN status(500).json({
        error: 'Internal server error',
        type: 'unknown'
    })
END TRY
```

---

## ğŸ“Š CONTEXT POLLUTION PREVENTION

### How Modes Stay Separate:

```
Request 1: AUTO_MODE
    â”œâ”€ Uses AUTO_MODE_PROMPT
    â”œâ”€ System: "You are a lecturer..."
    â””â”€ Response: Teaching-style

Request 2: CHAT_MODE (same session)
    â”œâ”€ Uses CHAT_MODE_PROMPT (NEW, independent)
    â”œâ”€ System: "You are a learning companion..."
    â””â”€ Response: Conversational-style

Key: Each request is STATELESS
     No conversation history on backend
     Fresh system prompt every time
```

---

## ğŸ§  PROMPT SELECTION LOGIC

```pseudocode
FUNCTION selectSystemPrompt(mode, hasNotes, hasSlideContext):
    basePrompt = NULL
    
    IF mode == 'AUTO_MODE':
        // Lecturer mode - fixed prompt
        basePrompt = AUTO_MODE_PROMPT
        // ~400 words, teaching-focused
        
    ELSE IF mode == 'CHAT_MODE':
        // Tutor mode - fixed prompt
        basePrompt = CHAT_MODE_PROMPT
        // ~350 words, exploration-focused
    END IF
    
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ADD SOFT CONTEXT (NOT RESTRICTIONS)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    augmentedPrompt = basePrompt
    
    IF hasNotes:
        notesContext = buildNotesContext(notes)
        augmentedPrompt += notesContext
        // Key phrase: "expand beyond them"
    END IF
    
    IF hasSlideContext AND mode == 'CHAT_MODE':
        slideContext = buildSlideContext(slideText, slideIndex)
        augmentedPrompt += slideContext
        // Key phrase: "just for context"
    END IF
    
    RETURN augmentedPrompt
END FUNCTION
```

---

## âš¡ PERFORMANCE OPTIMIZATION OPPORTUNITIES

### 1. Caching Strategy
```pseudocode
// Cache AUTO_MODE explanations per slide
slideExplanationCache = new Map()

IF mode == 'AUTO_MODE':
    cacheKey = generateHash(imageUrl + slideIndex)
    
    IF slideExplanationCache.has(cacheKey):
        RETURN slideExplanationCache.get(cacheKey)
    END IF
    
    response = await callOpenAI()
    slideExplanationCache.set(cacheKey, response)
    
    RETURN response
END IF
```

### 2. Model Selection Optimization
```pseudocode
// Use GPT-3.5 for simple CHAT_MODE follow-ups
IF mode == 'CHAT_MODE' AND isSimpleQuestion(userQuestion):
    model = "gpt-3.5-turbo"  // Faster, cheaper
ELSE:
    model = "gpt-4-turbo-preview"  // Better reasoning
END IF
```

### 3. Batch Processing (Future)
```pseudocode
// Pre-generate explanations for slide deck
FUNCTION pregenerateSlideExplanations(slides):
    FOR EACH slide IN slides:
        explanation = handleAutoSlideExplanation(slide)
        cache.set(slide.id, explanation)
    END FOR
END FUNCTION
```

---

## ğŸ” LOGGING & MONITORING

```pseudocode
FUNCTION handleAIChat(requestData):
    startTime = now()
    
    LOG.info({
        event: 'ai_request_received',
        mode: requestData.mode,
        hasImage: !!requestData.imageUrl,
        hasQuestion: !!requestData.userQuestion
    })
    
    TRY:
        result = processRequest(requestData)
        
        LOG.info({
            event: 'ai_request_success',
            mode: result.mode,
            responseLength: result.response.length,
            duration: now() - startTime
        })
        
        RETURN result
        
    CATCH error:
        LOG.error({
            event: 'ai_request_failed',
            mode: detectedMode,
            error: error.message,
            duration: now() - startTime
        })
        
        THROW error
    END TRY
END FUNCTION
```

---

**END OF PSEUDOCODE DOCUMENTATION**

This provides the complete backend logic flow for the dual-mode AI system.
